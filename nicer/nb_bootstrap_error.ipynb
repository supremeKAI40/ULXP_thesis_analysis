{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d497e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "from scipy.stats import uniform,norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cd973",
   "metadata": {},
   "source": [
    "Generate synthetic lightcurve with count rate being r'=r+gamma\\*sigma \n",
    "\n",
    "where sigma is flux error from original lc.*\n",
    "\n",
    "Code automatically scans nicer file directory for 9 digit obsid and computes on them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8357dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_light_curves(input_fits, output_folder, num_synthetic_curves):\n",
    "    # Extract observation ID from the input file path (assumes folder name is observation ID)\n",
    "    obs_id = input_fits.split('/')[1]\n",
    "\n",
    "    # Create a specific output directory for the synthetic light curves and best_fit_period.txt\n",
    "    output_dir = os.path.join(output_folder, obs_id)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Path for storing the best fit period file for the current obs_id\n",
    "    best_fit_period_file = os.path.join(output_dir, 'best_fit_period_0.1bin_100.txt')\n",
    "\n",
    "    # Open the input FITS file\n",
    "    with fits.open(input_fits) as hdul:\n",
    "        rate_data = hdul['RATE'].data\n",
    "        time = rate_data['TIME']\n",
    "        rate = rate_data['RATE']\n",
    "        error = rate_data['ERROR']\n",
    "\n",
    "\n",
    "        # Generate synthetic light curves and run efsearch\n",
    "        for i in range(num_synthetic_curves):\n",
    "            synthetic_rate = np.zeros_like(rate)  # Initialize array for synthetic rate\n",
    "            for j in range(len(rate)):\n",
    "                #gamma_j = np.random.uniform(-8, 8)  # Numpy uniform\n",
    "                gamma_j = uniform.rvs(loc=-error[j], scale=2*error[j])  # Scipy uniform\n",
    "                \n",
    "                synthetic_rate[j] = rate[j] + gamma_j*error[j] # Apply to each bin individually\n",
    "                # if(synthetic_rate[j])<0: ##Forcing 0 for gaussian variation negative values\n",
    "                #     synthetic_rate[j]=0\n",
    "            # Create a new synthetic FITS file\n",
    "            output_fits = os.path.join(output_dir, f'synthetic_light_curve_{i+1}.fits')\n",
    "\n",
    "            hdul_new = hdul.copy()\n",
    "            hdul_new['RATE'].data['RATE'] = synthetic_rate\n",
    "\n",
    "            hdul_new.writeto(output_fits, overwrite=True)\n",
    "\n",
    "            # Run efsearch on the synthetic light curve\n",
    "            efsearch_cmd = f'efsearch {output_fits} window=- sepoch=INDEF dper=9.81 nphase=64 nbint=INDEF dres=0.0001 nper=1024 plot=no outfile={output_folder}/{obs_id}/testing_efsearch.fes'\n",
    "            try:\n",
    "                subprocess.run(efsearch_cmd, shell=True, check=True)\n",
    "\n",
    "                # Open the .fes FITS file and extract the best-fit period\n",
    "                with fits.open(f'{output_folder}/{obs_id}/testing_efsearch.fes') as efsearch_fits:\n",
    "                    results_data = efsearch_fits['RESULTS'].data\n",
    "                    max_chisq_index = np.argmax(results_data['CHISQRD1'])\n",
    "                    best_fit_period = results_data['PERIOD'][max_chisq_index]\n",
    "                    chisq_max=results_data['CHISQRD1'][max_chisq_index]\n",
    "\n",
    "                # Write the light curve name and best-fit period to the individual output file\n",
    "                with open(best_fit_period_file, 'a') as f_out:\n",
    "                    f_out.write(f\"{output_fits}\\t{best_fit_period}\\t{chisq_max}\\n\")\n",
    "                print(f\"Processed {output_fits}, Best-fit period: {best_fit_period}\")\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error processing {output_fits}: {e}\")\n",
    "\n",
    "            # Delete the synthetic FITS file to save space\n",
    "            #os.remove(output_fits)\n",
    "            print(f\"Deleted synthetic light curve {i+1} to save space.\")\n",
    "\n",
    "def read_light_curve_paths(base_dir):\n",
    "    light_curve_paths = []\n",
    "\n",
    "    for obs_id in os.listdir(base_dir):\n",
    "        obs_dir = os.path.join(base_dir, obs_id)\n",
    "\n",
    "        if os.path.isdir(obs_dir):  # Check if it's a directory\n",
    "            input_fits_path = os.path.join(obs_dir, 'xti', 'event_cl', f'ni{obs_id}_cl_night_barycorrmpu7_sr_night_0.01.lc')\n",
    "\n",
    "            if os.path.isfile(input_fits_path):  # Check if the light curve file exists\n",
    "                light_curve_paths.append(input_fits_path)\n",
    "\n",
    "    return light_curve_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Base directory containing observation ID folders\n",
    "    base_dir = './'\n",
    "    output_folder = './bootstrap_error/'\n",
    "\n",
    "    # Number of synthetic light curves to generate\n",
    "    num_synthetic_curves = 10\n",
    "    # Read light curve paths\n",
    "    light_curve_paths = read_light_curve_paths(base_dir)\n",
    "\n",
    "        \n",
    "    # Generate synthetic light curves for all light curve files found and run efsearch\n",
    "    for path in light_curve_paths:\n",
    "        generate_synthetic_light_curves(path, output_folder, num_synthetic_curves)\n",
    "\n",
    "    print(\"Finished processing all light curves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec6add",
   "metadata": {},
   "source": [
    "### comparing light curves to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c8daf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_light_curves_different(rate1, rate2, tolerance=1e-6):\n",
    "    # Compute element-wise absolute difference\n",
    "    diff = np.abs(rate1 - rate2)\n",
    "    \n",
    "    # Check if all differences are smaller than the tolerance\n",
    "    if np.all(diff < tolerance):\n",
    "        return False  # They are the same within the tolerance\n",
    "    else:\n",
    "        return True   # They are different\n",
    "    \n",
    "# with fits.open('./bootstrap_error/6050390201/synthetic_light_curve_11.fits') as hdul:\n",
    "#         rate_data = hdul['RATE'].data\n",
    "#         time = rate_data['TIME']\n",
    "#         rate1 = rate_data['RATE']\n",
    "#         error1 = rate_data['ERROR']\n",
    "with fits.open('./6050390201_expt/xti/event_cl/ni6050390201_cl_night_barycorrmpu7_sr_night_0.1.lc') as hdul:\n",
    "        rate_data = hdul['RATE'].data\n",
    "        time = rate_data['TIME']\n",
    "        rate2 = rate_data['RATE']\n",
    "        error2 = rate_data['ERROR']\n",
    "\n",
    "# if are_light_curves_different(rate1, rate2):\n",
    "#     print(\"The two light curves are different.\")\n",
    "# else:\n",
    "#     print(\"The two light curves are identical or nearly identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52169d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.random.uniform(-1, 1, size=len(rate2))\n",
    "synthetic_rate=rate2+gamma*error2\n",
    "synthetic_rate2 = np.zeros_like(rate2)  # Initialize array for synthetic rate\n",
    "for j in range(len(rate2)):\n",
    "    gamma_j = np.random.uniform(-1, 1)  # Generate a new random number for each bin\n",
    "    synthetic_rate2[j] = rate2[j]+gamma_j*error2[j] \n",
    "\n",
    "gamma = np.random.uniform(-1, 1, size=len(rate2))\n",
    "synthetic_rate3=rate2+gamma*error2\n",
    "\n",
    "gamma = np.random.uniform(-1, 1, size=len(rate2))\n",
    "synthetic_rate4=rate2+gamma*error2\n",
    "\n",
    "synthetic_rate5 = np.zeros_like(rate2)  # Initialize array for synthetic rate\n",
    "for j in range(len(rate2)):\n",
    "    gamma_j = np.random.uniform(-1, 1)  # Generate a new random number for each bin\n",
    "    synthetic_rate5[j] = rate2[j]+gamma_j*error2[j] \n",
    "\n",
    "synthetic_rate6 = np.zeros_like(rate2)  # Initialize array for synthetic rate\n",
    "for j in range(len(rate2)):\n",
    "    gamma_j = np.random.normal(rate2[j], error2[j])  # Generate a new random number for each bin\n",
    "    synthetic_rate6[j] = gamma_j\n",
    "    if synthetic_rate6[j] < 0:\n",
    "        synthetic_rate6[j] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eba1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rate2,histtype='step',bins=10,label='original')\n",
    "plt.hist(synthetic_rate,histtype='step',bins=10, label='uniform')\n",
    "plt.hist(synthetic_rate3,histtype='step',bins=10, label='uniform')\n",
    "#plt.hist(synthetic_rate4,histtype='step',bins=10, label='uniform')\n",
    "##Using looped creation\n",
    "plt.hist(synthetic_rate2,histtype='step',bins=10, label='uniform')\n",
    "#plt.hist(synthetic_rate5,histtype='step',bins=10,label='uniform')\n",
    "plt.hist(synthetic_rate6,histtype='step',bins=10,label='normal')\n",
    "plt.legend()\n",
    "plt.title('Histogram of counts for different LC pre outburst')\n",
    "plt.xlabel('Values of rate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "82c7114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic_rate = np.zeros_like(rate2)\n",
    "# for j in range(len(rate2)):\n",
    "#     gamma_j = np.random.uniform(-1, 1)  # Ensure randomness in each bin\n",
    "#     synthetic_rate[j] = rate2[j] + gamma_j * error2[j]\n",
    "\n",
    "# # Add small random offset to the entire synthetic light curve\n",
    "# random_offset = np.random.normal(0, 0.01)\n",
    "# synthetic_rate += random_offset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.scatter(rate2,synthetic_rate,0.4,alpha=0.5)\n",
    "plt.scatter(rate2,synthetic_rate2,0.4,alpha=0.5)\n",
    "plt.scatter(rate2,synthetic_rate3,0.4,alpha=0.5)\n",
    "plt.scatter(rate2,synthetic_rate4,0.1, alpha=0.5)\n",
    "plt.grid()\n",
    "# plt.xlim([1000,1050])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1805a",
   "metadata": {},
   "source": [
    "## Diagnosing Randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "\n",
    "# Open the actual light curve FITS file\n",
    "with fits.open('./6050390204/xti/event_cl/ni6050390204_cl_night_barycorrmpu7_sr_night_0.01.lc') as hdul:\n",
    "    rate_data = hdul['RATE'].data\n",
    "    time = rate_data['TIME']  # Time array from FITS file\n",
    "    rate = rate_data['RATE']  # Original rate array\n",
    "    error = rate_data['ERROR']  # Error array\n",
    "\n",
    "# Number of synthetic curves\n",
    "num_synthetic_curves = 500  # Specify how many synthetic curves to generate\n",
    "synthetic_curves = []\n",
    "\n",
    "# Generate synthetic light curves\n",
    "for i in range(num_synthetic_curves):\n",
    "\n",
    "    # Generate random gamma values from a normal distribution\n",
    "    gamma_values = np.random.uniform(-1,1, size=len(rate))\n",
    "    #random_values=np.random.normal(0,1,size=len(rate))\n",
    "    \n",
    "    synthetic_rate = [max(r + g * e, 0) for r, g, e in zip(rate, gamma_values, error)]\n",
    "    synthetic_curves.append(synthetic_rate)\n",
    "\n",
    "# Convert synthetic curves to a NumPy array for easier manipulation\n",
    "synthetic_curves = np.array(synthetic_curves)\n",
    "\n",
    "# Check correlation between each synthetic light curve and the one just before it\n",
    "correlation_results = []\n",
    "spearman_correlations = []\n",
    "\n",
    "for i in range(1, num_synthetic_curves):\n",
    "    corr_coeff, _ = pearsonr(synthetic_curves[i], synthetic_curves[i - 1])\n",
    "    spearman_corr, _ = spearmanr(synthetic_curves[i], synthetic_curves[i - 1])\n",
    "    correlation_results.append(corr_coeff)\n",
    "    spearman_correlations.append(spearman_corr)\n",
    "    #print(f\"Correlation between Curve {i} and Curve {i-1}: {corr_coeff:.4f}\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e08dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(correlation_results)\n",
    "plt.title(\"Variation of Pearsonr coefficient from one curve to next\")\n",
    "plt.xlabel('no. of curves')\n",
    "plt.ylabel('pearsonr coeff.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_correlations=np.array(spearman_correlations)\n",
    "plt.plot(spearman_correlations[spearman_correlations>0.99878])\n",
    "plt.title(\"Variation of Spearman coefficient from one curve to next\")\n",
    "plt.xlabel('no. of curves')\n",
    "plt.ylabel('spearmanr coeff.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heasoftenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
